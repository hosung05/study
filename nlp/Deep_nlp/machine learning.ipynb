{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "73d4940103e11300043f00ad330523cc1a06dd889af7ec99652e23643049fdd9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "source": [
    "## 교차 엔트로피"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, P):\n",
    "    Y = np.array(Y)\n",
    "    P = np.array(P)\n",
    "    \n",
    "    return -np.sum(Y*np.log(P) + (1 - Y)*np.log(1 - P)).round(1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.8\n1.2\n"
     ]
    }
   ],
   "source": [
    "P = [[0.6, 0.2, 0.9, 0.3], [0.7, 0.9, 0.2, 0.4]]\n",
    "Y = [[1., 1., 0., 0.], [1., 1., 0., 0.]]\n",
    "\n",
    "for p, y in zip(P, Y):\n",
    "    print(cross_entropy(y, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "source": [
    "## 미니배치"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "source": [
    "라벨 인코딩인 경우 미니배치"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    betch_size = y.shape[0]\n",
    "\n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n2\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[2, 1]])\n",
    "t = np.array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "print(t.size)\n",
    "print(y.size)"
   ]
  },
  {
   "source": [
    "## 미분"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-7\n",
    "    return (f(x + h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-7\n",
    "    return (f(x + h) - f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.19999999989472883"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "source": [
    "## Gradient Descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-7\n",
    "\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        \n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        #f(x + h)\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        #f(x - h)\n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5.99999998, 8.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 100\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814392e-10])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-3.00000000e+00,  4.00000000e+00],\n",
       "       [-2.40000000e+00,  3.20000000e+00],\n",
       "       [-1.92000000e+00,  2.56000000e+00],\n",
       "       [-1.53600000e+00,  2.04800000e+00],\n",
       "       [-1.22880000e+00,  1.63840000e+00],\n",
       "       [-9.83040001e-01,  1.31072000e+00],\n",
       "       [-7.86432000e-01,  1.04857600e+00],\n",
       "       [-6.29145601e-01,  8.38860801e-01],\n",
       "       [-5.03316480e-01,  6.71088641e-01],\n",
       "       [-4.02653184e-01,  5.36870913e-01],\n",
       "       [-3.22122547e-01,  4.29496730e-01],\n",
       "       [-2.57698038e-01,  3.43597384e-01],\n",
       "       [-2.06158430e-01,  2.74877907e-01],\n",
       "       [-1.64926744e-01,  2.19902326e-01],\n",
       "       [-1.31941395e-01,  1.75921861e-01],\n",
       "       [-1.05553116e-01,  1.40737489e-01],\n",
       "       [-8.44424931e-02,  1.12589991e-01],\n",
       "       [-6.75539945e-02,  9.00719927e-02],\n",
       "       [-5.40431956e-02,  7.20575942e-02],\n",
       "       [-4.32345565e-02,  5.76460753e-02],\n",
       "       [-3.45876452e-02,  4.61168603e-02],\n",
       "       [-2.76701161e-02,  3.68934882e-02],\n",
       "       [-2.21360929e-02,  2.95147906e-02],\n",
       "       [-1.77088743e-02,  2.36118325e-02],\n",
       "       [-1.41670995e-02,  1.88894660e-02],\n",
       "       [-1.13336796e-02,  1.51115728e-02],\n",
       "       [-9.06694365e-03,  1.20892582e-02],\n",
       "       [-7.25355492e-03,  9.67140657e-03],\n",
       "       [-5.80284394e-03,  7.73712526e-03],\n",
       "       [-4.64227515e-03,  6.18970021e-03],\n",
       "       [-3.71382012e-03,  4.95176017e-03],\n",
       "       [-2.97105610e-03,  3.96140813e-03],\n",
       "       [-2.37684488e-03,  3.16912651e-03],\n",
       "       [-1.90147590e-03,  2.53530120e-03],\n",
       "       [-1.52118072e-03,  2.02824096e-03],\n",
       "       [-1.21694458e-03,  1.62259277e-03],\n",
       "       [-9.73555662e-04,  1.29807422e-03],\n",
       "       [-7.78844529e-04,  1.03845937e-03],\n",
       "       [-6.23075624e-04,  8.30767499e-04],\n",
       "       [-4.98460499e-04,  6.64613999e-04],\n",
       "       [-3.98768399e-04,  5.31691199e-04],\n",
       "       [-3.19014719e-04,  4.25352959e-04],\n",
       "       [-2.55211775e-04,  3.40282368e-04],\n",
       "       [-2.04169420e-04,  2.72225894e-04],\n",
       "       [-1.63335536e-04,  2.17780715e-04],\n",
       "       [-1.30668429e-04,  1.74224572e-04],\n",
       "       [-1.04534743e-04,  1.39379658e-04],\n",
       "       [-8.36277946e-05,  1.11503726e-04],\n",
       "       [-6.69022357e-05,  8.92029810e-05],\n",
       "       [-5.35217885e-05,  7.13623848e-05],\n",
       "       [-4.28174308e-05,  5.70899078e-05],\n",
       "       [-3.42539447e-05,  4.56719262e-05],\n",
       "       [-2.74031557e-05,  3.65375410e-05],\n",
       "       [-2.19225246e-05,  2.92300328e-05],\n",
       "       [-1.75380197e-05,  2.33840262e-05],\n",
       "       [-1.40304157e-05,  1.87072210e-05],\n",
       "       [-1.12243326e-05,  1.49657768e-05],\n",
       "       [-8.97946607e-06,  1.19726214e-05],\n",
       "       [-7.18357285e-06,  9.57809715e-06],\n",
       "       [-5.74685828e-06,  7.66247772e-06],\n",
       "       [-4.59748663e-06,  6.12998217e-06],\n",
       "       [-3.67798930e-06,  4.90398574e-06],\n",
       "       [-2.94239144e-06,  3.92318859e-06],\n",
       "       [-2.35391315e-06,  3.13855087e-06],\n",
       "       [-1.88313052e-06,  2.51084070e-06],\n",
       "       [-1.50650442e-06,  2.00867256e-06],\n",
       "       [-1.20520353e-06,  1.60693805e-06],\n",
       "       [-9.64162827e-07,  1.28555044e-06],\n",
       "       [-7.71330262e-07,  1.02844035e-06],\n",
       "       [-6.17064210e-07,  8.22752280e-07],\n",
       "       [-4.93651368e-07,  6.58201824e-07],\n",
       "       [-3.94921094e-07,  5.26561459e-07],\n",
       "       [-3.15936875e-07,  4.21249167e-07],\n",
       "       [-2.52749500e-07,  3.36999334e-07],\n",
       "       [-2.02199600e-07,  2.69599467e-07],\n",
       "       [-1.61759680e-07,  2.15679574e-07],\n",
       "       [-1.29407744e-07,  1.72543659e-07],\n",
       "       [-1.03526195e-07,  1.38034927e-07],\n",
       "       [-8.28209562e-08,  1.10427942e-07],\n",
       "       [-6.62567650e-08,  8.83423534e-08],\n",
       "       [-5.30054120e-08,  7.06738827e-08],\n",
       "       [-4.24043296e-08,  5.65391062e-08],\n",
       "       [-3.39234637e-08,  4.52312849e-08],\n",
       "       [-2.71387709e-08,  3.61850280e-08],\n",
       "       [-2.17110168e-08,  2.89480224e-08],\n",
       "       [-1.73688134e-08,  2.31584179e-08],\n",
       "       [-1.38950507e-08,  1.85267343e-08],\n",
       "       [-1.11160406e-08,  1.48213874e-08],\n",
       "       [-8.89283246e-09,  1.18571100e-08],\n",
       "       [-7.11426597e-09,  9.48568797e-09],\n",
       "       [-5.69141277e-09,  7.58855037e-09],\n",
       "       [-4.55313022e-09,  6.07084030e-09],\n",
       "       [-3.64250418e-09,  4.85667224e-09],\n",
       "       [-2.91400334e-09,  3.88533779e-09],\n",
       "       [-2.33120267e-09,  3.10827023e-09],\n",
       "       [-1.86496214e-09,  2.48661619e-09],\n",
       "       [-1.49196971e-09,  1.98929295e-09],\n",
       "       [-1.19357577e-09,  1.59143436e-09],\n",
       "       [-9.54860615e-10,  1.27314749e-09],\n",
       "       [-7.63888492e-10,  1.01851799e-09]])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 2.25899015e+09, -3.00990351e+09])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])  \n",
    "x, x_history = gradient_descent(function_2, init_x, lr=10, step_num=step_num)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])  \n",
    "x, x_history = gradient_descent(function_2, init_x, lr=1e-10, step_num=100)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    if exp_x.ndim == 1:\n",
    "        sum_exp_x = np.sum(exp_x)\n",
    "    else:\n",
    "        sum_exp_x = np.sum(exp_x, axis=1).reshape(-1, 1)\n",
    "    y = exp_x/sum_exp_x\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-14fdfd8bd6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "np.arange(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-7\n",
    "\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        \n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        #f(x + h)\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        #f(x - h)\n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.99406647,  2.10575318,  1.09262011],\n",
       "       [ 0.1190759 ,  1.192034  , -1.27453313]])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "net.W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.46449901, -0.82728162, -0.04572135],\n",
       "       [-1.03391311, -2.3273443 ,  0.37843057]])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "net.W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.6, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.2092212 , -2.59097885,  0.3131547 ])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.672985173636295"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.89147608, -0.52251708,  1.41399317],\n",
       "       [-1.33721413, -0.78377563,  2.12098974]])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "dw = numerical_gradient(f, net.W)\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.89147608, -0.52251708,  1.41399317],\n",
       "       [-1.33721413, -0.78377563,  2.12098974]])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "dw = numerical_gradient(lambda w: net.loss(x, t), net.W)\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.W -= 0.001 * dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.20748282, -2.58995994,  0.31039742])"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.66302181490844"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "\n",
    "        return T\n",
    "\n",
    "    with open('/Users/hosung/avangers/study/nlp/Deep_nlp/data/mnist_data/mnist.pkl', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(t == y)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "t_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(784, 50)\n(50,)\n(50, 10)\n(10,)\n"
     ]
    }
   ],
   "source": [
    "print(network.params['W1'].shape)\n",
    "print(network.params['b1'].shape)\n",
    "print(network.params['W2'].shape)\n",
    "print(network.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(50, 784)\n",
    "y = network.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50, 10)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "print(y.shape)\n",
    "np.argmax(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss : 2.2884560530576064 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.283587548178578 train acc, test acc | 0.10861666666666667, 0.1148\n",
      "loss : 2.2753023904485397 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.2927721680430495 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.283925916288285 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2909233232369934 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2785013817935535 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2899938076368254 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2792067137933514 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.307454890027154 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.295245961200526 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.287961707317824 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2951101629839403 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2996252614980075 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2761578957169575 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.290325046197205 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.313140266314108 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.2904561473584266 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.2923176189857415 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.30093381390339 train acc, test acc | 0.11246666666666667, 0.1131\n",
      "loss : 2.2918358401141123 train acc, test acc | 0.11238333333333334, 0.1135\n",
      "loss : 2.2792680251485913 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.293637246632392 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.29323175548334 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.285843999876427 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.28309602287522 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2938469746307106 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.3009175292502326 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2832387208271565 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.3018700844443676 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2933071851212006 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.299554811489595 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.283138953174459 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.276294533263209 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.293476967111891 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2922462993039034 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2903352973955005 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2877386177375123 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2875100449985495 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.298223802606133 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.278788819220489 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2929611526247813 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2957798754088152 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2999891089140743 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2834144065465645 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.296535250036303 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.3007542128067726 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2999588310243517 train acc, test acc | 0.10788333333333333, 0.1094\n",
      "loss : 2.283635185142108 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.290850517975628 train acc, test acc | 0.18025, 0.1811\n",
      "loss : 2.292798896300406 train acc, test acc | 0.16771666666666665, 0.1671\n",
      "loss : 2.2820685416161464 train acc, test acc | 0.09871666666666666, 0.098\n",
      "loss : 2.2912405499734723 train acc, test acc | 0.09871666666666666, 0.098\n",
      "loss : 2.290313714306346 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.2744861161796903 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.279226229751034 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.2820186494688963 train acc, test acc | 0.19831666666666667, 0.2038\n",
      "loss : 2.2639559006228005 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.263995520097176 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2888270901908223 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.2851864685227663 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.304153031658459 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2686661354767796 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.281900447497281 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.286318938689376 train acc, test acc | 0.11375, 0.1144\n",
      "loss : 2.2995411151407597 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.280596734492617 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2918311893374583 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.277225699963285 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2886276868161874 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2997461138918553 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2815138854027155 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.271392252620277 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.30129575166723 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.294410942666396 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2624797514564516 train acc, test acc | 0.15111666666666668, 0.1494\n",
      "loss : 2.274258830179985 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2961659094836087 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.275367695465206 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2977299267157956 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2842317376958174 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2837423448527274 train acc, test acc | 0.12318333333333334, 0.124\n",
      "loss : 2.2859498918148082 train acc, test acc | 0.1961, 0.1941\n",
      "loss : 2.284071232266229 train acc, test acc | 0.11238333333333334, 0.1135\n",
      "loss : 2.2651591836024436 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2950759103776623 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2870054194242435 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.269686426652566 train acc, test acc | 0.20678333333333335, 0.2057\n",
      "loss : 2.276019492940493 train acc, test acc | 0.14045, 0.1418\n",
      "loss : 2.2625215867291053 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2805557058920267 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.261443217866746 train acc, test acc | 0.11248333333333334, 0.1135\n",
      "loss : 2.246791961410399 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.296949811452895 train acc, test acc | 0.1542, 0.1552\n",
      "loss : 2.2584580220441275 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2867298845335564 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2647062098815134 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2854372477800493 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.295840168389368 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.283458975276555 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2771730191836737 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.291986566033948 train acc, test acc | 0.11245, 0.1137\n",
      "loss : 2.2761629462721267 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.284793807568821 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.276909740717685 train acc, test acc | 0.13311666666666666, 0.1351\n",
      "loss : 2.2801719930589206 train acc, test acc | 0.2082, 0.2092\n",
      "loss : 2.2766007633796628 train acc, test acc | 0.11271666666666667, 0.114\n",
      "loss : 2.269045282657384 train acc, test acc | 0.25988333333333336, 0.2545\n",
      "loss : 2.280004351492719 train acc, test acc | 0.1586, 0.1591\n",
      "loss : 2.2750589899443723 train acc, test acc | 0.15978333333333333, 0.1606\n",
      "loss : 2.261565402197708 train acc, test acc | 0.16056666666666666, 0.164\n",
      "loss : 2.236655531409498 train acc, test acc | 0.09741666666666667, 0.0983\n",
      "loss : 2.2693149990601307 train acc, test acc | 0.12563333333333335, 0.1283\n",
      "loss : 2.286582346820676 train acc, test acc | 0.09736666666666667, 0.0982\n",
      "loss : 2.27647215280475 train acc, test acc | 0.19206666666666666, 0.1918\n",
      "loss : 2.2812638495430435 train acc, test acc | 0.17961666666666667, 0.1822\n",
      "loss : 2.2579824408680174 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.28375362328738 train acc, test acc | 0.11851666666666667, 0.1217\n",
      "loss : 2.280353304164906 train acc, test acc | 0.17568333333333333, 0.1726\n",
      "loss : 2.268044250793703 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.272199469468767 train acc, test acc | 0.10893333333333333, 0.1091\n",
      "loss : 2.277899499299006 train acc, test acc | 0.12181666666666667, 0.1215\n",
      "loss : 2.2732878794283615 train acc, test acc | 0.10561666666666666, 0.1061\n",
      "loss : 2.2698766080388477 train acc, test acc | 0.11286666666666667, 0.1164\n",
      "loss : 2.2643057599366854 train acc, test acc | 0.11451666666666667, 0.1174\n",
      "loss : 2.2493629238578654 train acc, test acc | 0.1533, 0.1567\n",
      "loss : 2.2602750186816283 train acc, test acc | 0.34791666666666665, 0.3494\n",
      "loss : 2.2644434991848756 train acc, test acc | 0.10618333333333334, 0.1099\n",
      "loss : 2.2706480058053207 train acc, test acc | 0.19666666666666666, 0.1937\n",
      "loss : 2.254208989327351 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.2681988042943586 train acc, test acc | 0.10496666666666667, 0.1051\n",
      "loss : 2.253301822931786 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.2769471838820694 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.2599880475969947 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.2771881867326758 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.267819645152826 train acc, test acc | 0.1669, 0.1641\n",
      "loss : 2.2659084743964657 train acc, test acc | 0.19213333333333332, 0.1924\n",
      "loss : 2.2589592145368926 train acc, test acc | 0.1874, 0.1866\n",
      "loss : 2.2465787522785634 train acc, test acc | 0.2041, 0.2051\n",
      "loss : 2.2731601603872957 train acc, test acc | 0.28631666666666666, 0.2847\n",
      "loss : 2.2493370127347676 train acc, test acc | 0.28768333333333335, 0.2854\n",
      "loss : 2.2610287489421186 train acc, test acc | 0.22715, 0.2303\n",
      "loss : 2.262903741584942 train acc, test acc | 0.18245, 0.1804\n",
      "loss : 2.259752758518466 train acc, test acc | 0.19411666666666666, 0.193\n",
      "loss : 2.2517478789400003 train acc, test acc | 0.24728333333333333, 0.2459\n",
      "loss : 2.2472094492578973 train acc, test acc | 0.12546666666666667, 0.1239\n",
      "loss : 2.2419286795277746 train acc, test acc | 0.26461666666666667, 0.2606\n",
      "loss : 2.235454979407152 train acc, test acc | 0.16376666666666667, 0.1632\n",
      "loss : 2.258795246210599 train acc, test acc | 0.20121666666666665, 0.202\n",
      "loss : 2.2579213000841487 train acc, test acc | 0.1046, 0.1028\n",
      "loss : 2.237874211328258 train acc, test acc | 0.10463333333333333, 0.1034\n",
      "loss : 2.2617054027923613 train acc, test acc | 0.11401666666666667, 0.1134\n",
      "loss : 2.2444146147805424 train acc, test acc | 0.25253333333333333, 0.2515\n",
      "loss : 2.248817919191801 train acc, test acc | 0.19593333333333332, 0.1965\n",
      "loss : 2.248949487347988 train acc, test acc | 0.21095, 0.2118\n",
      "loss : 2.2409697391838233 train acc, test acc | 0.14301666666666665, 0.1469\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-63ba35c0297c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-c4de8a7f3b69>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-d6a5ecca7757>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, X)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_numerical_gradient_no_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-fd3bfc51c226>\u001b[0m in \u001b[0;36m_numerical_gradient_no_batch\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#f(x + h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mfxh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#f(x - h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-c4de8a7f3b69>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-c4de8a7f3b69>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-c4de8a7f3b69>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "\n",
    "    print(f\"loss : {loss} train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}